from python import csv
from python import numpy as np
from python import time
from python import argparse
from time_estimate import *

hash_URL = 0
hash_FileSize = 1
hash_Hash = 2
hash_FileType = 3
hash_FileLocation = 4
hash_FileName = 5
hash_DateCreated = 6
hash_DateModified = 7
hash_DriveName = 8


def dupefinder(hashcsvfile, dupefilepath):
    with open(hashcsvfile, "r") as hashfile:
        hashreader = csv.reader(hashfile)
        header = next(hashreader)
        hashes = list(hashreader)

    comparehashes = [row[3] for row in hashes if len(row) == len(header)]  # Assuming hash is in the 4th column

    with open(dupefilepath, "w", newline='') as dupefile:
        rofl = csv.writer(dupefile)
        startTime = time.time()
        for i, hash_row in enumerate(hashes):
            print(f"Processing line {i + 2}: {hash_row}")  # Adjust line number (1-indexed in Codon)

            hash_value = hash_row[3] if len(hash_row) == len(header) else None

            if hash_value is not None:
                matching_indices = [j for j, compare_hash_value in enumerate(comparehashes) if hash_value == compare_hash_value and i != j]

                for j in matching_indices:
                    compare_hash_url = hashes[j][1]  # Assuming URL is in the 2nd column
                    print(f"dupe detected: {hash_row[1]}, {compare_hash_url}")
                    rofl.writerow([hash_row[1], compare_hash_url, hash_row[2], hash_value])

    print(f"Generated {dupefilepath}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="Dupefinder",
                                     description="Given a CSV of file hashes generated by Hashmaker, will find all duplicates.  Will probably need a pass through dupe-dupe-checker afterwards.")
    parser.add_argument('hashcsvfile', help="Input CSV file containing hashes")
    parser.add_argument(
        'dupefilepath', help="Output CSV containing all duplicates")
    args = parser.parse_args()

    dupefinder(args.hashcsvfile, args.dupefilepath)
