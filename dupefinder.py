import csv, argparse, time, datetime
from time_estimate import *

hash_URL = 0
hash_FileSize = 1
hash_Hash = 2
hash_FileType = 3
hash_FileLocation = 4
hash_FileName = 5
hash_DateCreated = 6
hash_DateModified = 7
hash_DriveName = 8


"""Given a list of files generated by filetype-finder, will compare every file to every other file and compare their
hash value and filesize to determine if they're more than likely identical."""

def dupefinder(hashcsvfile, dupefilepath):

    with open(hashcsvfile, "r") as hashfile:
        hashes = []
        hashreader = csv.reader(hashfile)
        for line in hashreader:
            hashes.append(line)


    comparehashes = []
    comparesizes = []
    for j in range(0, len(hashes)):
        omfg = hashes[j]
        hash = omfg[hash_Hash]
        size = omfg[hash_FileSize]
        comparehashes.append(hash)
        comparesizes.append(size)

        if len(hash) != 40:
            print ("hash is not 40")
            for col in hashes[j]:
                print (col)
            
            break


    with open(dupefilepath, "w", newline='') as dupefile:
        rofl = csv.writer(dupefile)
        startTime = time.time()
        for i in range(0, len(hashes)):

            printTimeEstimate(i, len(hashes), startTime)
            hash = hashes[i]
            for j in range(0, len(hashes)):
                if i != j:
                    try:
                        if hash[hash_Hash] == comparehashes[j] and hash[hash_FileSize] == comparesizes[j]:
                            comparehash = hashes[j]
                            print("dupe detected: {name}, {name2}".format(name=hash[hash_URL], name2=comparehash[hash_URL]))
                            rofl.writerow([hash[hash_URL], comparehash[hash_URL], hash[hash_FileSize], hash[hash_Hash]])
                    except IndexError:
                        pass

    print("Generated {file}".format(file=dupefilepath))


if __name__ == "__main__":
    parser = argparse.ArgumentParser(prog="Dupefinder",
                                     description="Given a CSV of file hashes generated by Hashmaker, will find all duplicates.  Will probably need a pass through dupe-dupe-checker afterwards.")
    parser.add_argument('hashcsvfile', help="Input CSV file containing hashes")
    parser.add_argument(
        'dupefilepath', help="Output CSV containing all duplicates")
    args = parser.parse_args()

    dupefinder(args.hashcsvfile, args.dupefilepath)
